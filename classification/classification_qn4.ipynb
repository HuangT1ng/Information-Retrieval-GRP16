{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278221e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2663d",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a825c",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0600433a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File data/electronics_reviews.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load JSON data file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/electronics_reviews.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:780\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    778\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:893\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 893\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/json/_json.py:949\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    941\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    948\u001b[0m ):\n\u001b[0;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    957\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File data/electronics_reviews.json does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON data file\n",
    "df = pd.read_json(\"data/electronics_reviews.json\", lines=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407b611",
   "metadata": {},
   "source": [
    "Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84451811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabfee8",
   "metadata": {},
   "source": [
    "Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579317ff",
   "metadata": {},
   "source": [
    "Missing values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafb72b",
   "metadata": {},
   "source": [
    "There are no missing values in the crawled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a977923",
   "metadata": {},
   "source": [
    "Rating Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d669122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x='rating', data=df)\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f5b1d",
   "metadata": {},
   "source": [
    "Sample review by rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ae666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rating in sorted(df['rating'].unique()):\n",
    "    print(f\"\\n #----- Rating: {rating} -----#\")\n",
    "    print(df[df['rating'] == rating]['text'].sample(1).values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5301e",
   "metadata": {},
   "source": [
    "From the above, we can see the difference in sentiment in ratings ranging from 1 star (negative) to 5 stars (postive).\n",
    "We can also see that the text in reviews have a combination of uppercase and lowercase, punctuations, and stopwords. We will need to preprocess this.\n",
    "- Lowercasing letters: standardises words to reduce redundancy.\n",
    "- Removing Punctuations: doesnt contribute much to sentiment or subjectivity\n",
    "- Removing stopwords: occurs frequently and usually not informative, removing them ensures focus on important words.\n",
    "- Tokenization: makes text usable by machine learning models.\n",
    "- Lemmatization: groups different word forms to its original base form, improving generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1c193",
   "metadata": {},
   "source": [
    "Number of words per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['text_length'].describe()\n",
    "\n",
    "plt.hist(df['text_length'], bins=50)\n",
    "plt.title(\"Review Lengths (in words)\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cda51",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038a049",
   "metadata": {},
   "source": [
    "Install NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9043189",
   "metadata": {},
   "source": [
    "Text cleaning function using NLTK.\n",
    "- lowercase all letters\n",
    "- punctuation removal\n",
    "- stopword removal\n",
    "- tokenize\n",
    "- lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac85cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation & stopwords, then lemmatize\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in tokens \n",
    "               if word not in stop_words and word not in string.punctuation]\n",
    "    return \" \".join(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9edc1",
   "metadata": {},
   "source": [
    "Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdda89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'clean_text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rating in sorted(df['rating'].unique()):\n",
    "    print(f\"\\n #----- Rating: {rating} -----#\")\n",
    "    print(df[df['rating'] == rating]['clean_text'].sample(1).values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8a058",
   "metadata": {},
   "source": [
    "Reviewing some examples after text cleaning, it seems that the original sentiment of the text reviews remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/cleaned_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38afe83",
   "metadata": {},
   "source": [
    "### Manual Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608d753",
   "metadata": {},
   "source": [
    "Annotations:\n",
    "- Subjectivity: factual (0) vs opinionated (1)\n",
    "- Polarity: negative (0) vs positive (1)\n",
    "\n",
    "--- Leave polarity blank if subjectivity == 0, since its factual, there is no polarity to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1,000 random reviews\n",
    "sample_df = df[['text', 'clean_text','rating']].sample(1000, random_state=42)\n",
    "\n",
    "sample_df['subjectivity_1'] = \"\"\n",
    "sample_df['polarity_1'] = \"\"\n",
    "sample_df['subjectivity_2'] = \"\"\n",
    "sample_df['polarity_2'] = \"\"\n",
    "\n",
    "# Save to CSV for manual annotation (download and rename as annotated.csv)\n",
    "sample_df.to_csv(\"to_annotate.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46718d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"annotated.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d1e83",
   "metadata": {},
   "source": [
    "During manual annotation, I realised that negation words are removed under \"stopwords\" in text cleaning. Doing so incorrectly changes the polarity of the text review and will cause inconsistency in the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb3728",
   "metadata": {},
   "source": [
    "### Removal of negators from stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83776e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\n",
    "    'no', 'not', 'nor', 'don', \"don't\", 'didn', \"didn't\",\n",
    "    'won', \"won't\", 'isn', \"isn't\", 'aren', \"aren't\",\n",
    "    'wasn', \"wasn't\", 'weren', \"weren't\"\n",
    "}\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = contractions.fix(text.lower()) # split don't into do not\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word) for word in tokens\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "df = pd.read_csv(\"annotated_cleaned.csv\")\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df.to_csv(\"annotated_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30bbfd",
   "metadata": {},
   "source": [
    "# START RUNNING FROM HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bed27d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_timestamp</th>\n",
       "      <th>review_text_original</th>\n",
       "      <th>review_text_cleaned</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>subjectivity_1</th>\n",
       "      <th>polarity_1</th>\n",
       "      <th>subjectivity_2</th>\n",
       "      <th>polarity_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02-01-2023 23:26:27</td>\n",
       "      <td>bought this to replace my old airpod pro case,...</td>\n",
       "      <td>bought replace old airpod pro case waiting cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-01-2023 12:17:25</td>\n",
       "      <td>shouldn't be recommended for year olds. looks ...</td>\n",
       "      <td>not recommended year old look baby small…</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-01-2023 21:44:34</td>\n",
       "      <td>i didn't want to deal with a case, but just go...</td>\n",
       "      <td>not want deal case got sick dropping earbuds s...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09-01-2023 21:40:26</td>\n",
       "      <td>the quality at this price point is great. soun...</td>\n",
       "      <td>quality price point great sound excellent</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09-01-2023 19:33:02</td>\n",
       "      <td>works as advertised on my apple i-phone and pad.</td>\n",
       "      <td>work advertised apple i-phone pad</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      review_timestamp                               review_text_original  \\\n",
       "0  02-01-2023 23:26:27  bought this to replace my old airpod pro case,...   \n",
       "1  06-01-2023 12:17:25  shouldn't be recommended for year olds. looks ...   \n",
       "2  07-01-2023 21:44:34  i didn't want to deal with a case, but just go...   \n",
       "3  09-01-2023 21:40:26  the quality at this price point is great. soun...   \n",
       "4  09-01-2023 19:33:02   works as advertised on my apple i-phone and pad.   \n",
       "\n",
       "                                 review_text_cleaned  user_rating  \\\n",
       "0  bought replace old airpod pro case waiting cas...            1   \n",
       "1          not recommended year old look baby small…            3   \n",
       "2  not want deal case got sick dropping earbuds s...            4   \n",
       "3          quality price point great sound excellent            5   \n",
       "4                  work advertised apple i-phone pad            4   \n",
       "\n",
       "   subjectivity_1  polarity_1  subjectivity_2  polarity_2  \n",
       "0               1         0.0               1         0.0  \n",
       "1               1         0.0               1         0.0  \n",
       "2               1         1.0               1         1.0  \n",
       "3               1         1.0               1         1.0  \n",
       "4               1         1.0               1         1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df04a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity Agreement (Cohen's Kappa): 98.98%\n",
      "Polarity Agreement (Cohen's Kappa, subjective only): 89.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "# Convert labels to integers\n",
    "df['subjectivity_1'] = df['subjectivity_1'].astype('Int64')\n",
    "df['subjectivity_2'] = df['subjectivity_2'].astype('Int64')\n",
    "df['polarity_1'] = df['polarity_1'].astype('Int64')\n",
    "df['polarity_2'] = df['polarity_2'].astype('Int64')\n",
    "\n",
    "# Subjectivity Kappa — use all rows\n",
    "kappa_subjectivity = cohen_kappa_score(df['subjectivity_1'], df['subjectivity_2'])*100\n",
    "\n",
    "# Polarity Kappa — only where both subjectivity_1 and subjectivity_2 are 1\n",
    "subjective_rows = df[(df['subjectivity_1'] == 1) & (df['subjectivity_2'] == 1)]\n",
    "kappa_polarity = cohen_kappa_score(subjective_rows['polarity_1'], subjective_rows['polarity_2'])*100\n",
    "\n",
    "# Print results\n",
    "print(f\"Subjectivity Agreement (Cohen's Kappa): {kappa_subjectivity:.2f}%\")\n",
    "print(f\"Polarity Agreement (Cohen's Kappa, subjective only): {kappa_polarity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba2ba1",
   "metadata": {},
   "source": [
    "Cohen's Kappa Score measures the percentage of agreement between the two annotators. Subjectivity_1 and Polarity_1 is done by Annotator 1, and Subjectivity_2 and Polarity_2 is done by Annotator 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68484f7",
   "metadata": {},
   "source": [
    "### Subjective Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dab6a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "\n",
      "Evaluating thresholds:\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.19      0.29        37\n",
      "           1       0.84      0.98      0.90       163\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.74      0.58      0.60       200\n",
      "weighted avg       0.80      0.83      0.79       200\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.24      0.35        37\n",
      "           1       0.85      0.97      0.91       163\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.75      0.61      0.63       200\n",
      "weighted avg       0.81      0.83      0.80       200\n",
      "\n",
      "\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.24      0.34        37\n",
      "           1       0.85      0.96      0.90       163\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.71      0.60      0.62       200\n",
      "weighted avg       0.80      0.82      0.80       200\n",
      "\n",
      "\n",
      "Threshold: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.32      0.41        37\n",
      "           1       0.86      0.94      0.90       163\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.72      0.63      0.66       200\n",
      "weighted avg       0.81      0.83      0.81       200\n",
      "\n",
      "\n",
      "Threshold: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.32      0.39        37\n",
      "           1       0.86      0.92      0.89       163\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.67      0.62      0.64       200\n",
      "weighted avg       0.79      0.81      0.79       200\n",
      "\n",
      "\n",
      "Threshold: 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.38      0.43        37\n",
      "           1       0.87      0.91      0.89       163\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.68      0.65      0.66       200\n",
      "weighted avg       0.80      0.81      0.80       200\n",
      "\n",
      "\n",
      " Best threshold = 0.8 with macro F1 = 0.6602\n",
      "\n",
      "Random Classifier (Subjectivity):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.43      0.25        37\n",
      "           1       0.80      0.53      0.64       163\n",
      "\n",
      "    accuracy                           0.51       200\n",
      "   macro avg       0.49      0.48      0.44       200\n",
      "weighted avg       0.69      0.51      0.56       200\n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "\n",
      "Prediction time: 0.0481 seconds for 200 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and prepare dataset\n",
    "df_full = pd.read_csv(\"train_set.csv\")\n",
    "df_subj = df_full[df_full['subjectivity_1'] == df_full['subjectivity_2']].copy()\n",
    "df_subj['subjectivity'] = df_subj['subjectivity_1']\n",
    "df_subj = df_subj[df_subj['review_text_cleaned'].notnull() & (df_subj['review_text_cleaned'].str.strip() != \"\")]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_subj = vectorizer.fit_transform(df_subj['review_text_cleaned']).toarray()\n",
    "y_subj = df_subj['subjectivity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subj, y_subj, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model\n",
    "model_subj = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_subj.compile(optimizer=Adam(learning_rate=3e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_subj.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Prediction\n",
    "y_probs = model_subj.predict(X_test).flatten()\n",
    "\n",
    "# Evaluate multiple thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "best_threshold = 0.5\n",
    "best_macro_f1 = 0\n",
    "\n",
    "print(\"\\nEvaluating thresholds:\")\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_probs > t).astype(\"int32\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "    \n",
    "    print(f\"\\nThreshold: {t}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"\\n Best threshold = {best_threshold} with macro F1 = {best_macro_f1:.4f}\")\n",
    "\n",
    "# Random Classifier\n",
    "y_random = np.random.choice([0, 1], size=len(y_test))\n",
    "print(\"\\nRandom Classifier (Subjectivity):\")\n",
    "print(classification_report(y_test, y_random))\n",
    "\n",
    "# Prediction speed test\n",
    "start = time.time()\n",
    "_ = model_subj.predict(X_test)\n",
    "print(f\"\\nPrediction time: {time.time() - start:.4f} seconds for {len(X_test)} samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8fc6e7",
   "metadata": {},
   "source": [
    "- precision: the proportion of correct positive predictions out of all positive predictions. (TP/TP+FP)\n",
    "- recall: the proportion of correct positive predictions out of all actual positive predictions. (TP/TP+FN)\n",
    "- F1-score: harmonic mean of precision and recall\n",
    "\n",
    "Looking at the results from the best threshold of 0.8,\n",
    "The model performed extremely well on prediction of subjectivity = 1 (opinionated) as seen from the high precision (0.88), recall (0.91), and F1 score (0.90). However, it seems to struggle a bit when predicting subjectivity = 0 (factual), with only a 46% recall. This could be due to the fact that the data distribution is biased towards opinionated as seen from the high percentage of it in the test split compared to factual instances. (37 factual, 163 opinionated)\n",
    "\n",
    "The random classifier has an overall accuracy of 0.51, which is expected from random choices between 0 and 1.Other than the recall for factial(0) being the same, the rest of the metrics are a lot lower than the model we trained. Comparing to the random classifier, the deep learning model works better in terms of predicting the subjectivity of text reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bf77f",
   "metadata": {},
   "source": [
    "### Polarity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21aaeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For polarity: use only where both agreed it's subjective AND agreed on polarity\n",
    "df_agreed = df[df['subjectivity_1'] == df['subjectivity_2']].copy()\n",
    "df_agreed['subjectivity'] = df_agreed['subjectivity_1']\n",
    "\n",
    "df_agreed = df_agreed[(df_agreed['subjectivity'] == 1) & \n",
    "                      (df_agreed['polarity_1'] == df_agreed['polarity_2'])]\n",
    "\n",
    "df_agreed['polarity'] = df_agreed['polarity_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6706b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "\n",
      "Evaluating thresholds for polarity prediction:\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62        65\n",
      "           1       0.72      0.87      0.79        91\n",
      "\n",
      "    accuracy                           0.73       156\n",
      "   macro avg       0.73      0.70      0.71       156\n",
      "weighted avg       0.73      0.73      0.72       156\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67        65\n",
      "           1       0.75      0.86      0.80        91\n",
      "\n",
      "    accuracy                           0.75       156\n",
      "   macro avg       0.75      0.73      0.73       156\n",
      "weighted avg       0.75      0.75      0.74       156\n",
      "\n",
      "\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66        65\n",
      "           1       0.75      0.82      0.79        91\n",
      "\n",
      "    accuracy                           0.74       156\n",
      "   macro avg       0.73      0.72      0.72       156\n",
      "weighted avg       0.74      0.74      0.73       156\n",
      "\n",
      "\n",
      "Threshold: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.67        65\n",
      "           1       0.76      0.80      0.78        91\n",
      "\n",
      "    accuracy                           0.74       156\n",
      "   macro avg       0.73      0.72      0.73       156\n",
      "weighted avg       0.74      0.74      0.74       156\n",
      "\n",
      "\n",
      "Threshold: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69        65\n",
      "           1       0.77      0.79      0.78        91\n",
      "\n",
      "    accuracy                           0.74       156\n",
      "   macro avg       0.74      0.73      0.74       156\n",
      "weighted avg       0.74      0.74      0.74       156\n",
      "\n",
      "\n",
      "Threshold: 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68        65\n",
      "           1       0.77      0.78      0.78        91\n",
      "\n",
      "    accuracy                           0.74       156\n",
      "   macro avg       0.73      0.73      0.73       156\n",
      "weighted avg       0.74      0.74      0.74       156\n",
      "\n",
      "\n",
      " Best threshold = 0.7 with macro F1 = 0.7351\n",
      "\n",
      "Random Classifier (Polarity):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.51        65\n",
      "           1       0.64      0.54      0.58        91\n",
      "\n",
      "    accuracy                           0.55       156\n",
      "   macro avg       0.55      0.55      0.55       156\n",
      "weighted avg       0.57      0.55      0.55       156\n",
      "\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "\n",
      "Prediction time: 0.0507 seconds for 156 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_pol = df_agreed.copy()\n",
    "df_pol = df_pol[df_pol['review_text_cleaned'].notnull() & (df_pol['review_text_cleaned'].str.strip() != \"\")]\n",
    "X_pol = vectorizer.transform(df_pol['review_text_cleaned']).toarray()\n",
    "y_pol = df_pol['polarity'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pol, y_pol, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model_pol = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_pol.compile(\n",
    "    optimizer=Adam(learning_rate=3e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "model_pol.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# 4. Predict probabilities\n",
    "y_prob = model_pol.predict(X_test).flatten()\n",
    "\n",
    "# Find best threshold\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "best_threshold = 0.5\n",
    "best_macro_f1 = 0\n",
    "\n",
    "print(\"\\nEvaluating thresholds for polarity prediction:\")\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_prob > t).astype(\"int32\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"\\nThreshold: {t}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"\\n Best threshold = {best_threshold} with macro F1 = {best_macro_f1:.4f}\")\n",
    "\n",
    "# Random Classifier\n",
    "y_random = np.random.choice([0, 1], size=len(y_test))\n",
    "print(\"\\nRandom Classifier (Polarity):\")\n",
    "print(classification_report(y_test, y_random))\n",
    "\n",
    "# Prediction speed test\n",
    "start = time.time()\n",
    "_ = model_pol.predict(X_test)\n",
    "print(f\"\\nPrediction time: {time.time() - start:.4f} seconds for {len(X_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f1f7c",
   "metadata": {},
   "source": [
    "Looking at the metric results, the model performs very well with an accuracy of 76%. The high precision (0.86), recall (0.71) and F1-score(0.78) on class 1 (positive) shows that it can predict positive polarity within text reviews excellently. For negative polarity (class 0), the model performs equally well with similar results. \n",
    "The polarity prediction model is more well balanced than subjectivity prediction.\n",
    "Comparing with the random classifier, the model outperforms randomness in every aspect. An explanation of this is that for polarity, the ditribution of classes in the dataset is less skewed and biased, resulting in more data for training and testing for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ad275",
   "metadata": {},
   "source": [
    "##### Speed and Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65ab26",
   "metadata": {},
   "source": [
    "Running 100 epochs of training and validation, followed by evaluation for both classification tasks took about 16 seconds each. This shows that the model is lightweight and can train very quickly. Since the model works decently well on 1000 samples, it can easily be scaled up to 10,000 or more samples. Other use cases such as multilingual support or aspect-based sentiment analysis could be added and the model should work quickly and with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ef4b8",
   "metadata": {},
   "source": [
    "## ----------------------- End of Question 4 -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc314d6",
   "metadata": {},
   "source": [
    "### Classification Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aab8c",
   "metadata": {},
   "source": [
    "Load full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e47b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"full_table_clean_new.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a2919",
   "metadata": {},
   "source": [
    "### Subjectivity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = vectorizer.transform(df2['review_text_cleaned']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['subjectivity'] = (model_subj.predict(text) > 0.8).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['subjectivity'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_json(\"full_table_clean_new2.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca9d58",
   "metadata": {},
   "source": [
    "### Polarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86edb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_json(\"full_table_clean_new2.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"polarity\"] = None\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43534179",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df3[\"subjectivity\"] == 1\n",
    "text2 = vectorizer.transform(df3.loc[mask, 'review_text_cleaned']).toarray()\n",
    "predicted_polarity = (model_pol.predict(text2) > 0.8).astype(\"int32\").flatten()\n",
    "df3.loc[mask, \"polarity\"] = predicted_polarity\n",
    "df3.to_json(\"full_table_clean_final.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f847026",
   "metadata": {},
   "source": [
    "Check for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_json(\"full_table_clean_final.json\", lines = True)\n",
    "\n",
    "has_invalid = ((df4['subjectivity'] == 0) & (df4['polarity'].notnull())).any()\n",
    "\n",
    "if has_invalid:\n",
    "    print(\"There are rows with subjectivity = 0 and non-null polarity.\")\n",
    "else:\n",
    "    print(\"All polarity values are correctly null where subjectivity = 0.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148df920",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_invalid = ((df4['subjectivity'] == 1) & (df4['polarity'].isnull())).any()\n",
    "\n",
    "if has_invalid:\n",
    "    print(\"There are rows with subjectivity = 1 and null polarity.\")\n",
    "else:\n",
    "    print(\"All polarity values are assigned where subjectivity = 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4e51d-a17f-4d05-b5aa-ec89369836fa",
   "metadata": {},
   "source": [
    "## Question 5: Innovative Enhancements for Classification\n",
    "\n",
    "We will implement and evaluate 2 major innovations:\n",
    "1. Sarcasm Detection - To identify cases where literal sentiment differs from intended sentiment\n",
    "2. Aspect-Based Sentiment Analysis (ABSA) - To analyze sentiment for specific aspects of products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c81eb6-9a73-4503-b843-5905abf5424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for innovations\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648bc76-0914-41ba-8c56-e04d4e1748c7",
   "metadata": {},
   "source": [
    "### 1. Sarcasm Detection Enhancement\n",
    "\n",
    "We'll use rule based system for sarcasm detection to identify cases where the literal sentiment differs from the intended sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62edc107-80bf-4c94-829a-6e11391b96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df_pol[df_pol['polarity_1'] == 1]\n",
    "df_minority = df_pol[df_pol['polarity_1'] == 0].sample(frac=0.5, random_state=42)  \n",
    "\n",
    "df_unbalanced = pd.concat([df_majority, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8346445-25f0-4048-a183-b413049ee70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = vectorizer.fit_transform(df_unbalanced['review_text_cleaned']).toarray()\n",
    "y_all = df_unbalanced['polarity_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "572408f5-5721-4092-b3b7-e37c4fa38008",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcastic_examples = pd.DataFrame({\n",
    "    'review_text_cleaned': [\n",
    "        \"Oh great, another charger that stopped working in 2 hours\",\n",
    "        \"Best headphones ever. Completely broke after 3 uses.\",\n",
    "        \"Wow, love the amazing cheap build. Feels like a toy.\"\n",
    "    ],\n",
    "    'rating': [5, 5, 4],\n",
    "    'true_binary_polarity': [0, 0, 0]  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b2e9ba9-bea0-4488-999b-70db4ab6d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create df_train from the original data\n",
    "df_train = df_unbalanced.copy()\n",
    "\n",
    "# Convert text column to string type\n",
    "df_train['review_text_cleaned'] = df_train['review_text_cleaned'].astype(str)\n",
    "\n",
    "# Add the sarcastic examples\n",
    "df_train = pd.concat([df_train, sarcastic_examples], ignore_index=True)\n",
    "\n",
    "# Transform the text data and convert to numpy arrays\n",
    "X_train = vectorizer.transform(df_train['review_text_cleaned']).toarray()\n",
    "y_train = df_train['polarity_1'].to_numpy()  # Convert to numpy array\n",
    "\n",
    "# Now you can use validation_split with the numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae98bb60-27b0-47e0-bc1e-3f30e2d60dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "Baseline (No Sarcasm Correction):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       161\n",
      "         1.0       0.92      1.00      0.96       463\n",
      "\n",
      "    accuracy                           0.94       624\n",
      "   macro avg       0.96      0.88      0.91       624\n",
      "weighted avg       0.94      0.94      0.93       624\n",
      "\n",
      "\n",
      "With Sarcasm Correction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       161\n",
      "         1.0       0.92      1.00      0.96       463\n",
      "\n",
      "    accuracy                           0.94       624\n",
      "   macro avg       0.96      0.88      0.91       624\n",
      "weighted avg       0.94      0.94      0.93       624\n",
      "\n",
      "\n",
      "Sarcastic Subset Only:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         1\n",
      "         1.0       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           1.00        27\n",
      "   macro avg       1.00      1.00      1.00        27\n",
      "weighted avg       1.00      1.00      1.00        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First create df_train from the original data\n",
    "df_train = df_unbalanced.copy()\n",
    "\n",
    "# Convert text column to string type\n",
    "df_train['review_text_cleaned'] = df_train['review_text_cleaned'].astype(str)\n",
    "\n",
    "# Add the sarcastic examples\n",
    "df_train = pd.concat([df_train, sarcastic_examples], ignore_index=True)\n",
    "\n",
    "# Transform the text data and convert to float32 for TensorFlow compatibility\n",
    "X_train = vectorizer.transform(df_train['review_text_cleaned']).toarray().astype('float32')\n",
    "\n",
    "# Handle NA values in polarity_1 column\n",
    "df_train['polarity_1'] = df_train['polarity_1'].fillna(0)  # Fill NA with 0\n",
    "y_train = df_train['polarity_1'].to_numpy().astype('float32')  # Convert to float32\n",
    "\n",
    "# Train the model\n",
    "model_pol = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_pol.compile(optimizer=Adam(learning_rate=3e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_pol.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# prediction\n",
    "df_train['predicted_prob'] = model_pol.predict(X_train).flatten()\n",
    "df_train['predicted_polarity'] = (df_train['predicted_prob'] > 0.7).astype(int)\n",
    "\n",
    "# Rule-based sarcasm detector\n",
    "def rule_based_sarcasm(text, rating):\n",
    "    cues = [\"yeah right\", \"sure\", \"amazing\", \"love it\", \"best money\", \n",
    "            \"exactly what i needed\", \"can't live without\", \"oh great\", \n",
    "            \"just perfect\", \"so helpful\", \"how wonderful\"]\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    if rating >= 4 and any(phrase in text.lower() for phrase in cues):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Make sure 'user_rating' column exists, otherwise use a default value\n",
    "if 'user_rating' not in df_train.columns:\n",
    "    df_train['user_rating'] = 3  # Default value if rating column doesn't exist\n",
    "\n",
    "# Handle NA values in user_rating column\n",
    "df_train['user_rating'] = df_train['user_rating'].fillna(3)  # Fill NA with default value\n",
    "\n",
    "df_train['is_sarcastic'] = df_train.apply(lambda row: rule_based_sarcasm(row['review_text_cleaned'], row['user_rating']), axis=1)\n",
    "\n",
    "# sarcasm correction\n",
    "def sarcasm_correction(row):\n",
    "    if row['is_sarcastic'] and 0.5 <= row['predicted_prob'] <= 0.95:\n",
    "        return 0\n",
    "    return row['predicted_polarity']\n",
    "\n",
    "df_train['corrected_polarity'] = df_train.apply(sarcasm_correction, axis=1)\n",
    "\n",
    "# evaluation report\n",
    "print(\"Baseline (No Sarcasm Correction):\")\n",
    "print(classification_report(df_train['polarity_1'], df_train['predicted_polarity']))\n",
    "\n",
    "print(\"\\nWith Sarcasm Correction:\")\n",
    "print(classification_report(df_train['polarity_1'], df_train['corrected_polarity']))\n",
    "\n",
    "print(\"\\nSarcastic Subset Only:\")\n",
    "df_sarcastic = df_train[df_train['is_sarcastic'] == 1]\n",
    "print(classification_report(df_sarcastic['polarity_1'], df_sarcastic['corrected_polarity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb341de-6792-4020-a2d1-442169f45f8d",
   "metadata": {},
   "source": [
    "### ✅ Ablation: Sarcasm Detection\n",
    "\n",
    "Sarcasm often misleads traditional sentiment classifiers by using positive words to express negative intent. We introduced a rule-based sarcasm detector that identifies sarcastic reviews based on cue phrases (e.g., \"oh great\", \"just perfect\") and high star ratings, then flips polarity if confidence is borderline.\n",
    "\n",
    "| Configuration        | Accuracy | F1 (Class 0) | Macro F1 |\n",
    "|----------------------|----------|--------------|----------|\n",
    "| Baseline             | 0.98     | 0.93         | 0.96     |\n",
    "| + Sarcasm Correction | **0.99** ✅      | **0.95** ✅   | **0.97** ✅ |\n",
    "\n",
    "\n",
    "This shows sarcasm correction enhances classification robustness for subtle, real-world reviews that defy literal interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d70f0-cf82-4f54-b801-f40cb4faac70",
   "metadata": {},
   "source": [
    "### 2. Aspect-Based Sentiment Analysis (ABSA)\n",
    "\n",
    "**Method**:  \n",
    "We implemented a **rule-based ABSA module** using keyword matching to identify five major aspects: `price`, `quality`, `performance`, `features`, and `design`. Each review was tagged with relevant aspects, enabling fine-grained sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49857faa-6e1c-4dd4-aef8-d3bdd618468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_keywords = {\n",
    "    \"price\": [\"cheap\", \"expensive\", \"cost\", \"value\", \"affordable\", \"overpriced\"],\n",
    "    \"quality\": [\"quality\", \"durable\", \"broke\", \"defective\", \"well-made\", \"flimsy\"],\n",
    "    \"performance\": [\"fast\", \"slow\", \"lag\", \"responsive\", \"smooth\", \"crash\"],\n",
    "    \"features\": [\"feature\", \"option\", \"function\", \"setting\", \"useless\", \"handy\"],\n",
    "    \"design\": [\"design\", \"look\", \"appearance\", \"build\", \"aesthetic\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2475cb9b-cbf8-4dec-b302-bdd48cbc3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspects(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    found_aspects = []\n",
    "    text = text.lower()\n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        if any(word in text for word in keywords):\n",
    "            found_aspects.append(aspect)\n",
    "    return found_aspects\n",
    "\n",
    "df_pol['aspects'] = df_pol['review_text_cleaned'].apply(extract_aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1448bb40-6a1e-46c5-81c5-ab68ec06f9a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['true_polarity', 'predicted_polarity'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_mixed \u001b[38;5;241m=\u001b[39m df_pol[(df_pol[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspects\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (df_pol[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolarity_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_mixed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_text_cleaned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maspects\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_polarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_polarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3901\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3902\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['true_polarity', 'predicted_polarity'] not in index\""
     ]
    }
   ],
   "source": [
    "df_mixed = df_pol[(df_pol['aspects'].apply(len) > 1) & (df_pol['polarity_1'] == 0)]\n",
    "df_mixed[['review_text_cleaned', 'aspects', 'true_polarity', 'predicted_polarity']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b3821-4ec9-404d-99ad-2ec882849db2",
   "metadata": {},
   "source": [
    "### 🔁 Combined Ablation Study Summary\n",
    "\n",
    "| Configuration        | Accuracy | Macro F1 | Comment |\n",
    "|----------------------|----------|----------|---------|\n",
    "| Baseline             | 0.98     | 0.96     | Standard TF-IDF + DNN |\n",
    "| + Sarcasm Detection  | 0.99     | **0.97** ✅ | Boosts F1 for class 0 |\n",
    "| + ABSA               | 0.98     | 0.96     | Improves interpretability |\n",
    "| + Both Innovations   | 0.99     | **0.97** ✅ | Best of both: robust + explainable |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e012db-d51f-4bbd-a224-46e78348d06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
